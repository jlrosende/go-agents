# model: &model azure.o3-mini
# model: &model generic.qwen3
model: &model openai.o3-mini

agents:
  agent_one:
    # url: "http://localhost:8000/"
    model: *model
    instructions: |
      You are an AI assistant
    servers:
      - filesystem
    include_tools:
      - read_file
    request_params:
      use_history: true
      max_iterations: 20
      max_tokens: 8196
      parallel_tool_calls: true
      # temperature: 0.7

mcp:
  servers:
    filesystem:
      command: "npx"
      args: ["-y", "@modelcontextprotocol/server-filesystem", "."]
    fetch:
      command: "uvx"
      args: ["mcp-server-fetch"]

openai:
  # base_url: https://api.business.githubcopilot.com/
  base_url: https://api.githubcopilot.com/

azure:
  api_version: "2024-12-01-preview"

generic:
  api_key: ollama
  base_url: http://ollama:11434/v1/

logger:
  type: "console" # "console", "file"
  level: "debug" # "debug", "info", "warn", "error", "fatal"
  path: "agent.jsonl" # Path to log file (for "file" type)
