model: &model openai.o3-mini

agents:
  agent_one:
    url: "http://localhost:8000/"
    model: *model
    instructions: |
      You are a AI asistant
    servers: 
      - filesystem
    include_tools: 
      - read_file

mcp:
  servers:
    filesystem:
      command: "npx"
      args: ["-y", "@modelcontextprotocol/server-filesystem", "."]
    fetch:
      command: "uvx"
      args: ["mcp-server-fetch"]

openai:
  # base_url: https://api.business.githubcopilot.com/
  base_url: https://api.githubcopilot.com/

azure:
  api_version: "2024-12-01-preview"

generic:
  api_key: ollama
  base_url: http://ollama:11434/v1

logger:
  type: "console"  # "console", "file"
  level: "debug"  # "debug", "info", "warn", "error", "fatal"
  path: "agent.jsonl"  # Path to log file (for "file" type)