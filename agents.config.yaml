# model: &model azure.o3-mini
# model: &model generic.qwen3
model: &model openai.o3-mini.high
# model: &model openai.o4-mini.high

agents:
  agent_one:
    # transport: "http" "sse"
    # url: "http://localhost:8000/mcp"
    model: *model
    instructions: |
      Yo are a AI assistant
    servers:
      - filesystem
      # - fetch
    include_tools:
      - read_file
      - write_file
      - edit_file
      # - fetch
    request_params:
      use_history: true
      max_iterations: 20
      max_tokens: 8196
      parallel_tool_calls: false
      temperature: 0
      reasoning: true

mcp:
  servers:
    filesystem:
      command: "npx"
      args: ["-y", "@modelcontextprotocol/server-filesystem", "."]
    fetch:
      command: "uvx"
      args: ["mcp-server-fetch"]

openai:
  #   base_url: https://api.business.githubcopilot.com/
  base_url: https://api.githubcopilot.com/
  # base_url: https://api.openai.com/v1

azure:
  api_version: "2024-12-01-preview"

generic:
  api_key: ollama
  base_url: http://ollama:11434/v1/

logger:
  type: "console" # "console", "file"
  level: "debug" # "debug", "info", "warn", "error", "fatal"
  path: "agent.jsonl" # Path to log file (for "file" type)
