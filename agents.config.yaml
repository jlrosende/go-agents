model: openai.o3-mini

agents:
  agent_one:
    url: "http://localhost:8000/"
    model: openai.o3-mini
    instructions: |
      asasasas
    servers: 
      - filesystem
    include_tools:
      - read_file
    exclude_tools:
      - write_file
    request_params:
      temperature: 0.7
      max_tokens: 8192
      use_history: false
      max_iterations: 20
      parallel_tool_calls: true
      response_format: {} # any
      template_vars: {} # map[string]any
mcp:
  servers:
    filesystem:
      command: "npx"
      args: ["-y", "@modelcontextprotocol/server-filesystem", "."]
    fetch:
      command: "uvx"
      args: ["mcp-server-fetch"]

openai:
  # base_url: https://api.business.githubcopilot.com/
  base_url: https://api.githubcopilot.com/

azure:
  api_version: "2024-12-01-preview"
  azure_deployment: "o3-mini"

generic:
  api_key: ollama
  base_url: http://ollama:11434/v1
